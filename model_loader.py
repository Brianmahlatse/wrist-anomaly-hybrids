# -*- coding: utf-8 -*-
"""model_loader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x9eKbNqiXSFYO1WvdnF75los4ZvHFRMD
"""

"""
model_loader.py

Utility functions to:
1. Build the CNN and ViT backbones used in the paper
   (Xception, DenseNet201, DeiT-base-distilled, ViT-B/16).
2. Assemble Parallel or Sequential hybrid models.
3. Load trained checkpoints.

This file is meant to let other researchers reproduce
the evaluation of our saved models on new wrist X-rays.

We intentionally do not hardcode any private paths.
Callers must supply their own checkpoint locations.
"""

import copy
import torch

from hybrid_vision import (
    XceptionClassifier,
    DenseNet201Classifier,
    DeiTClassifier,
    ViTClassifier,
    CNNExtractor,
    ViTFeatureLayer,
    ParallelHybridClassifier,
    SequentialHybridClassifier,
)


def _device():
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")


def build_backbones_pair(pair_name):
    """
    Returns a tuple (cnn_model, vit_model) for a given backbone pair.

    pair_name can be:
    - "xc_deit"  -> (XceptionClassifier, DeiTClassifier)
    - "dn_vit"   -> (DenseNet201Classifier, ViTClassifier)

    The returned models are unmodified, just initialized with
    their ImageNet / pretrained transformer weights (via timm / HF).
    """
    if pair_name == "xc_deit":
        cnn = XceptionClassifier()
        vit = DeiTClassifier()
    elif pair_name == "dn_vit":
        cnn = DenseNet201Classifier()
        vit = ViTClassifier()
    else:
        raise ValueError(
            f"Unknown pair_name '{pair_name}'. "
            "Expected 'xc_deit' or 'dn_vit'."
        )
    return cnn, vit


def load_backbone_weights(cnn_model, vit_model, cnn_ckpt_path=None, vit_ckpt_path=None, map_location="cpu"):
    """
    Optionally load fine-tuned weights for the standalone CNN and ViT backbones
    before hybrid assembly.

    cnn_ckpt_path and vit_ckpt_path are .pth or .pt state_dict files.
    Pass None to skip loading for that backbone.
    """
    if cnn_ckpt_path is not None:
        state = torch.load(cnn_ckpt_path, map_location=map_location)
        cnn_model.load_state_dict(state)

    if vit_ckpt_path is not None:
        state = torch.load(vit_ckpt_path, map_location=map_location)
        vit_model.load_state_dict(state)

    return cnn_model, vit_model


def build_hybrid(
    fusion_type,
    cnn_model,
    vit_model,
    dropout_p=0.1,
    num_labels=1,
):
    """
    Create either a Parallel or Sequential hybrid model using the
    provided CNN and ViT backbones.

    fusion_type: "parallel" or "sequential"
    cnn_model:   an instance of XceptionClassifier or DenseNet201Classifier
    vit_model:   an instance of DeiTClassifier or ViTClassifier
    dropout_p:   dropout to apply in ViTFeatureLayer
    num_labels:  output dimension (1 for binary classification)

    Returns:
        model (nn.Module) in eval() mode and moved to device
    """

    # Deep copy the base models so multiple hybrids don't share weights
    cnn_extractor = CNNExtractor(copy.deepcopy(cnn_model))
    vit_extractor = ViTFeatureLayer(copy.deepcopy(vit_model), dropout_p=dropout_p)

    if fusion_type.lower() in ["parallel", "par"]:
        hybrid = ParallelHybridClassifier(
            cnn_extractor=cnn_extractor,
            vit_extractor=vit_extractor,
            num_labels=num_labels,
        )
    elif fusion_type.lower() in ["sequential", "seq"]:
        hybrid = SequentialHybridClassifier(
            cnn_extractor=cnn_extractor,
            vit_extractor=vit_extractor,
            num_labels=num_labels,
        )
    else:
        raise ValueError("fusion_type must be 'parallel'/'par' or 'sequential'/'seq'.")

    # Move to device and run a dummy forward pass to trigger lazy head creation
    dev = _device()
    hybrid = hybrid.to(dev).eval()

    with torch.no_grad():
        dummy = torch.zeros(2, 3, 224, 224, device=dev)
        _ = hybrid(dummy)

    return hybrid


def load_hybrid_from_checkpoint(
    fusion_type,
    pair_name,
    hybrid_ckpt_path,
    cnn_ckpt_path=None,
    vit_ckpt_path=None,
    map_location="cpu",
    dropout_p=0.1,
    num_labels=1,
):
    """
    High level convenience helper.

    1. Build the requested backbone pair (xc_deit or dn_vit).
    2. (Optional) load standalone CNN/ViT weights if provided.
       For example, weights fine-tuned on wrist data.
    3. Build the requested hybrid (parallel or sequential).
    4. Load the trained hybrid checkpoint.

    Args:
        fusion_type:   "parallel" / "par" or "sequential" / "seq"
        pair_name:     "xc_deit" or "dn_vit"
        hybrid_ckpt_path: path to the trained hybrid .pth state_dict
        cnn_ckpt_path: optional fine-tuned CNN weights
        vit_ckpt_path: optional fine-tuned ViT weights
        map_location:  default "cpu"
        dropout_p:     dropout for ViTFeatureLayer
        num_labels:    output classes (1 for binary)

    Returns:
        model (nn.Module) ready for inference on the current device.
    """

    # 1. init backbones
    cnn_model, vit_model = build_backbones_pair(pair_name)

    # 2. load fine-tuned standalone weights if available
    cnn_model, vit_model = load_backbone_weights(
        cnn_model,
        vit_model,
        cnn_ckpt_path=cnn_ckpt_path,
        vit_ckpt_path=vit_ckpt_path,
        map_location=map_location,
    )

    # 3. assemble hybrid
    model = build_hybrid(
        fusion_type=fusion_type,
        cnn_model=cnn_model,
        vit_model=vit_model,
        dropout_p=dropout_p,
        num_labels=num_labels,
    )

    # 4. load trained hybrid checkpoint
    state = torch.load(hybrid_ckpt_path, map_location=map_location)
    model.load_state_dict(state)

    return model.eval()