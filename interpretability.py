# -*- coding: utf-8 -*-
"""interpretability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vyNgG1gRCpE1qs0_QuAxWhpZUCRq4K4o
"""

"""
interpretability.py

Utility functions for generating visual explanations from CNNâ€“ViT hybrid
wrist anomaly detectors. These functions were used to produce the
saliency overlays and attention maps reported in the paper.

Contents:
  - get_vit_attention: Attention rollout / attention-based relevance map
    from the Transformer branch (CLS token backprojection).
  - layercam / layercam_multi: LayerCAM-style gradient-weighted
    activation maps from CNN layers.
  - overlay_heat / to_uint8_img: Utilities for producing overlays.

References:
  - Jetley et al., 2018. Learn to Pay Attention.
  - Selvaraju et al., 2019. Grad-CAM: Visual Explanations from Deep Networks
    via Gradient-based Localization.
  - Jiang et al., 2021. LayerCAM: Exploring Hierarchical Class Activation Map
    for Localization.

These implementations are provided for interpretability and reproducibility.
"""

# model_interpretability.py

import math, inspect
from collections import deque
from typing import Optional, Callable, Tuple, Union, List

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from matplotlib import pyplot as plt

TensorLike = Union[np.ndarray, torch.Tensor]


# ----------------------------
# utils
# ----------------------------
def _get_logits(out: Union[dict, torch.Tensor]) -> torch.Tensor:
    if isinstance(out, dict):
        if "logits" in out:
            return out["logits"]
        for v in out.values():
            if isinstance(v, torch.Tensor) and v.ndim >= 2:
                return v
        raise ValueError("No logits in output dict")
    if isinstance(out, torch.Tensor):
        return out
    raise ValueError(f"Unsupported output type {type(out)}")


def _to_tensor_nchw(x: TensorLike) -> torch.Tensor:
    t = torch.from_numpy(x) if isinstance(x, np.ndarray) else x
    if t.ndim == 2:
        t = t.unsqueeze(0).unsqueeze(0).repeat(1, 3, 1, 1)
    elif t.ndim == 3:
        if t.shape[0] in (1, 3):
            t = t.unsqueeze(0)
            if t.shape[1] == 1:
                t = t.repeat(1, 3, 1, 1)
        else:
            t = t.permute(2, 0, 1).unsqueeze(0)
    elif t.ndim != 4:
        raise ValueError(f"Unsupported input shape {tuple(t.shape)}")
    return t.float()


def _get_hw(x: TensorLike) -> Tuple[int, int]:
    a = x if isinstance(x, torch.Tensor) else np.asarray(x)
    if a.ndim == 2:
        return a.shape[-2], a.shape[-1]
    if a.ndim == 3:
        if isinstance(x, torch.Tensor) and x.shape[0] in (1, 3):
            return a.shape[-2], a.shape[-1]
        return a.shape[-3], a.shape[-2]
    if a.ndim == 4:
        return a.shape[-2], a.shape[-1]
    raise ValueError("Could not infer H,W")


def to_uint8_img(x: TensorLike) -> np.ndarray:
    t = torch.from_numpy(x) if isinstance(x, np.ndarray) else x
    if t.ndim == 3 and t.shape[0] in (1, 3):
        t = t.permute(1, 2, 0)
    t = t.detach().cpu().float()
    if t.ndim == 2:
        t = t.unsqueeze(-1).repeat(1, 1, 3)
    t_min, t_max = t.min(), t.max()
    if t_min < 0 or t_max > 1:
        t = (t - t_min) / (t_max - t_min + 1e-8)
    return (t.clamp(0, 1).numpy() * 255.0).astype(np.uint8)


def overlay_heat(img_uint8: np.ndarray, heatmap: TensorLike, cmap: str = "viridis", alpha: float = 0.5) -> np.ndarray:
    from matplotlib import cm
    h = heatmap.detach().cpu().numpy() if torch.is_tensor(heatmap) else np.asarray(heatmap)
    h = (h - h.min()) / (h.max() - h.min() + 1e-8)
    heat_rgb = (cm.get_cmap(cmap)(h)[..., :3] * 255.0).astype(np.uint8)
    out = (img_uint8.astype(np.float32) * (1 - alpha) + heat_rgb.astype(np.float32) * alpha).clip(0, 255)
    return out.astype(np.uint8)


# ----------------------------
# unwrap helper
# ----------------------------
def _unwrap_hf_vit(module: nn.Module) -> nn.Module:
    """
    Try very hard to find the real HF ViT/DeiT inside user wrappers.
    We accept:
      1) a module whose config.model_type is deit/vit/beit/vision-transformer
      2) a module whose forward(...) has output_attentions
    If nothing is found we just return the original module.
    """
    q, seen = deque([module]), set()
    while q:
        m = q.popleft()
        if id(m) in seen:
            continue
        seen.add(id(m))

        cfg = getattr(m, "config", None)
        if cfg is not None:
            mt = getattr(cfg, "model_type", None)
            if mt in ("deit", "vit", "beit", "vision-transformer"):
                return m

        try:
            sig = inspect.signature(m.forward)
            if "output_attentions" in sig.parameters:
                return m
        except Exception:
            pass

        for name in ("model", "backbone", "vit", "deit", "encoder", "transformer"):
            if hasattr(m, name):
                sub = getattr(m, name)
                if isinstance(sub, nn.Module):
                    q.append(sub)

        for c in m.children():
            q.append(c)

    return module  # fallback


# ----------------------------
# attention rollout
# ----------------------------
@torch.no_grad()
def get_vit_attention(
    vit_backbone: nn.Module,
    img: TensorLike,
    discard_ratio: float = 0.1,
    preprocess: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
    device: Optional[torch.device] = None,
) -> torch.Tensor:
    """
    Attention rollout that survives:
    - hybrids and feature wrappers
    - HF sdpa default
    - DeiT with distillation token
    """
    # 1) always unwrap first
    vit_backbone = _unwrap_hf_vit(vit_backbone)

    # 2) force HF to give us attentions
    cfg = getattr(vit_backbone, "config", None)
    if cfg is not None:
        cfg.return_dict = True
        # hit both, some HF versions check the private field
        cfg._attn_implementation = "eager"
        cfg.attn_implementation = "eager"
        cfg.output_attentions = True

    # 3) preprocess image
    H, W = _get_hw(img)
    x = _to_tensor_nchw(img)
    if preprocess is not None:
        x = preprocess(x)
    if device is None:
        device = next(vit_backbone.parameters()).device
    x = x.to(device)
    vit_backbone.eval()

    # 4) call HF DeiT/Vit in the canonical way
    out = vit_backbone(pixel_values=x, output_attentions=True, return_dict=True)
    atts = getattr(out, "attentions", None)
    if not atts or atts[0].ndim != 4:
        raise RuntimeError("No attentions")

    # 5) rollout
    T = atts[0].shape[-1]
    result = torch.eye(T, dtype=torch.float32, device=x.device)
    for att in atts:
        a = att[0].mean(dim=0)  # [T, T]
        if discard_ratio and discard_ratio > 0:
            flat = a.reshape(-1)
            k = int(round(discard_ratio * flat.numel()))
            if 0 < k < flat.numel():
                cutoff = torch.sort(flat, descending=True).values[k]
                a = torch.where(a < cutoff, torch.zeros_like(a), a)
        a = a + torch.eye(T, device=x.device, dtype=a.dtype)
        a = a / a.sum(dim=-1, keepdim=True)
        result = a @ result

    # 6) drop class token (and distil token if present)
    drop = 2 if int(math.isqrt(T - 2)) ** 2 == (T - 2) else 1
    vec = result[0, drop:]
    M = vec.numel()
    n = int(math.isqrt(M))
    if n * n != M:
        raise ValueError(f"Token grid not square. Got {M}")
    grid = vec.reshape(1, 1, n, n)
    grid = F.interpolate(grid, size=(H, W), mode="bilinear", align_corners=False)[0, 0]
    grid = (grid - grid.min()) / (grid.max() - grid.min() + 1e-8)
    return grid.detach().cpu()


# ----------------------------
# LayerCAM
# ----------------------------
def layercam(model: nn.Module, x: torch.Tensor, target_layer: nn.Module, class_index: Optional[int] = None) -> np.ndarray:
    model.eval()
    device = next(model.parameters()).device
    if x.ndim == 3:
        x = x.unsqueeze(0)
    x = x.to(device)
    B, C, H, W = x.shape
    assert B == 1
    acts, grads = [], []

    def fwd_hook(m, i, o):
        acts.append(o)

    def bwd_hook(m, gi, go):
        grads.append(go[0])

    h1 = target_layer.register_forward_hook(fwd_hook)
    h2 = target_layer.register_full_backward_hook(bwd_hook)
    try:
        out = model(x)
        logits = _get_logits(out)
        if logits.ndim != 2:
            raise ValueError(f"Bad logits shape {tuple(logits.shape)}")
        if class_index is None:
            class_index = 0 if logits.shape[1] == 1 else int(torch.argmax(logits, dim=1).item())
        score = logits[:, class_index].sum()
        model.zero_grad(set_to_none=True)
        score.backward()
        A, G = acts[0], grads[0]
        A = A.clamp(min=0)
        G = G.clamp(min=0)
        cam = (A * G).sum(dim=1, keepdim=True)
        cam = F.interpolate(cam, size=(H, W), mode="bilinear", align_corners=False)[0, 0]
        cam = cam - cam.min()
        cam = cam / (cam.max() + 1e-6)
        return cam.detach().cpu().numpy()
    finally:
        h1.remove()
        h2.remove()


def pick_last_k_convs(module: nn.Module, k: int = 3, include_1x1: bool = False):
    convs = []
    for _, m in module.named_modules():
        if isinstance(m, nn.Conv2d):
            if include_1x1:
                convs.append(m)
            else:
                ks = m.kernel_size if isinstance(m.kernel_size, tuple) else (m.kernel_size, m.kernel_size)
                if ks != (1, 1):
                    convs.append(m)
    if not convs:
        raise ValueError("No Conv2d layers found")
    return convs[-k:]  # deepest k, kept in shallow-to-deep order


def layercam_multi(model, x, target_layers, class_index=None, weights=None, combine="mean", return_all=False):
    maps = [torch.from_numpy(layercam(model, x, L, class_index=class_index)) for L in target_layers]
    M = torch.stack(maps, dim=0)
    if weights is not None:
        w = torch.tensor(weights, dtype=M.dtype)
        w = w / max(w.sum().item(), 1e-6)
        M = M * w.view(-1, 1, 1)
    if combine == "max":
        fused = M.max(dim=0).values
    elif combine == "sum":
        fused = M.sum(dim=0)
    else:
        fused = M.mean(dim=0)
    fused = fused - fused.min()
    fused = fused / max(fused.max().item(), 1e-6)
    if return_all:
        per_layer = []
        for t in maps:
            tt = t - t.min()
            tt = tt / max(tt.max().item(), 1e-6)
            per_layer.append(tt.numpy())
        return fused.numpy(), per_layer
    return fused.numpy()
