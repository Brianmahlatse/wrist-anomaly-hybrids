# -*- coding: utf-8 -*-
"""hybrid_vision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rHNCCFII867Jd-MHre4e28FdTHs2ToN3
"""

"""
hybrid_vision.py

Core model components used in the paper:
1. CNN classifiers (Xception, DenseNet201)
2. ViT / DeiT classifiers
3. Feature extractors for CNN and ViT backbones
4. Parallel and Sequential CNNâ€“ViT hybrid architectures

These classes are sufficient to:
- Recreate the hybrid models used in the study
- Load pretrained checkpoints
- Run inference and evaluation

Author: [Your name]
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import timm
from transformers import DeiTModel, ViTModel


# -------------------------
# Backbone classifiers
# -------------------------

class XceptionClassifier(nn.Module):
    """
    CNN baseline using an ImageNet pretrained Xception-style backbone (timm 'legacy_xception').
    Outputs a single logit for binary classification.
    """
    def __init__(self, dropout: float = 0.3):
        super().__init__()
        self.backbone = timm.create_model(
            "legacy_xception",
            pretrained=True,
            num_classes=0,          # no classifier head
            global_pool="avg"
        )
        in_feats = self.backbone.num_features
        self.classifier = nn.Sequential(
            nn.Linear(in_feats, 1024),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(1024, 1)
        )
        self.loss_fn = nn.BCEWithLogitsLoss()

    def forward(self, pixel_values: torch.Tensor, labels: torch.Tensor = None):
        feats = self.backbone(pixel_values)          # [B, F]
        logits = self.classifier(feats)              # [B, 1]

        out = {"logits": logits}
        if labels is not None:
            labels = labels.float().view(-1, 1)
            out["loss"] = self.loss_fn(logits, labels)
        return out


class DenseNet201Classifier(nn.Module):
    """
    CNN baseline using an ImageNet pretrained DenseNet-201 backbone.
    Same head shape as XceptionClassifier for consistency.
    """
    def __init__(self, dropout: float = 0.3):
        super().__init__()
        self.backbone = timm.create_model(
            "densenet201.tv_in1k",
            pretrained=True,
            num_classes=0,
            global_pool="avg"
        )
        in_feats = self.backbone.num_features
        self.classifier = nn.Sequential(
            nn.Linear(in_feats, 1024),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(1024, 1)
        )
        self.loss_fn = nn.BCEWithLogitsLoss()

    def forward(self, pixel_values: torch.Tensor, labels: torch.Tensor = None):
        feats = self.backbone(pixel_values)          # [B, F]
        logits = self.classifier(feats)              # [B, 1]

        out = {"logits": logits}
        if labels is not None:
            labels = labels.float().view(-1, 1)
            out["loss"] = self.loss_fn(logits, labels)
        return out


class DeiTClassifier(nn.Module):
    """
    Transformer baseline using a DeiT backbone (distilled DeiT-base).
    Returns a single logit for binary classification.
    """
    def __init__(self, model_name: str = "facebook/deit-base-distilled-patch16-224", dropout: float = 0.3):
        super().__init__()
        self.vit = DeiTModel.from_pretrained(model_name, ignore_mismatched_sizes=True)
        self.head = nn.Sequential(
            nn.Linear(768, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(512, 1)
        )
        self.loss_fn = nn.BCEWithLogitsLoss()

    def forward(self, pixel_values: torch.Tensor, labels: torch.Tensor = None):
        cls_token = self.vit(pixel_values=pixel_values).last_hidden_state[:, 0, :]  # [B, 768]
        logits = self.head(cls_token)                                               # [B, 1]

        out = {"logits": logits}
        if labels is not None:
            out["loss"] = self.loss_fn(logits, labels)
        return out


class ViTClassifier(nn.Module):
    """
    Transformer baseline using a ViT backbone (e.g. ViT-B/16).
    Mirrors DeiTClassifier but swaps the pretrained backbone.
    """
    def __init__(self, model_name: str = "google/vit-base-patch16-224", dropout: float = 0.3):
        super().__init__()
        self.vit = ViTModel.from_pretrained(model_name, ignore_mismatched_sizes=True)
        self.head = nn.Sequential(
            nn.Linear(768, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(512, 1)
        )
        self.loss_fn = nn.BCEWithLogitsLoss()

    def forward(self, pixel_values: torch.Tensor, labels: torch.Tensor = None):
        cls_token = self.vit(pixel_values=pixel_values).last_hidden_state[:, 0, :]  # [B, 768]
        logits = self.head(cls_token)                                               # [B, 1]

        out = {"logits": logits}
        if labels is not None:
            out["loss"] = self.loss_fn(logits, labels)
        return out


# -------------------------------------------------
# Freezing strategy for partial fine-tuning
# -------------------------------------------------

def set_trainable_fraction(
    model: nn.Module,
    kind: str = "vit",
    fraction: float = 0.6,
    freeze_embeddings: bool = True,
    keep_stem_frozen: bool = True,
):
    """
    Partially unfreezes only the last `fraction` of the backbone blocks.

    kind="vit": works with HuggingFace DeiT / ViT or timm ViT.
    kind="cnn": works with timm CNN backbones.

    After this call:
    - most of the backbone is frozen
    - classifier / head layers are still trainable
    """
    # unwrap DataParallel if present
    if hasattr(model, "module"):
        model = model.module

    # freeze everything first
    for p in model.parameters():
        p.requires_grad = False

    if kind == "vit":
        vit = getattr(model, "vit", None)
        blocks = None

        if vit is not None:
            # HuggingFace structure
            if hasattr(vit, "encoder") and hasattr(vit.encoder, "layer"):
                blocks = list(vit.encoder.layer)
            # timm-style ViT
            elif hasattr(vit, "blocks"):
                blocks = list(vit.blocks)

        if blocks is None:
            raise ValueError("ViT blocks not found")

        n = len(blocks)
        k = max(1, int(round(fraction * n)))
        cutoff = n - k

        for i, blk in enumerate(blocks):
            if i >= cutoff:
                for p in blk.parameters():
                    p.requires_grad = True

        # final norms / pooler
        for mod in [getattr(vit, "layernorm", None), getattr(vit, "pooler", None)]:
            if mod is not None:
                for p in mod.parameters():
                    p.requires_grad = True

        # optionally unfreeze embeddings
        if not freeze_embeddings and hasattr(vit, "embeddings"):
            for p in vit.embeddings.parameters():
                p.requires_grad = True

    elif kind == "cnn":
        bb = getattr(model, "backbone", model)
        if hasattr(bb, "blocks"):
            blocks = list(bb.blocks)
        elif hasattr(bb, "features"):
            blocks = list(bb.features)
        else:
            blocks = [m for m in bb.children()]

        n = len(blocks)
        k = max(1, int(round(fraction * n)))
        cutoff = n - k

        for i, blk in enumerate(blocks):
            if i >= cutoff:
                for p in blk.parameters():
                    p.requires_grad = True

        if not keep_stem_frozen and hasattr(bb, "stem"):
            for p in bb.stem.parameters():
                p.requires_grad = True

    else:
        raise ValueError("kind must be 'vit' or 'cnn'")

    # always keep classifier / head trainable
    for name in ["classifier", "head"]:
        if hasattr(model, name):
            for p in getattr(model, name).parameters():
                p.requires_grad = True


# -------------------------------------------------
# Feature extraction blocks used by the hybrids
# -------------------------------------------------

class CNNExtractor(nn.Module):
    """
    Wraps a CNN classifier or backbone and returns a pooled feature vector [B, C].
    Removes the final classification head and exposes features only.
    """
    def __init__(self, model: nn.Module):
        super().__init__()
        # Some classifiers store the CNN in .backbone, others are the backbone directly
        self.backbone = getattr(model, "backbone", model)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # timm CNNs often expose forward_features()
        if hasattr(self.backbone, "forward_features"):
            out = self.backbone.forward_features(x)

            # timm DenseNet / EfficientNet style outputs
            if isinstance(out, dict) and "global_pool" in out:
                return out["global_pool"]  # [B, C]

            # if we still have [B, C, H, W], apply global pooling
            if isinstance(out, torch.Tensor) and out.ndim == 4:
                out = F.adaptive_avg_pool2d(out, 1).flatten(1)
            return out  # [B, C]

        # fallback: just call the module, assume it returns embeddings
        return self.backbone(x)


class ViTFeatureLayer(nn.Module):
    def __init__(self, vit_model, dropout_p=0.1):
        super().__init__()
        # vit_model can be a plain HF DeiT/ViT or a wrapper with .vit
        self.vit = getattr(vit_model, "vit", vit_model)
        self.drop = nn.Dropout(dropout_p)

    def forward(self, x, **kwargs):
        # Hugging Face style models
        if hasattr(self.vit, "config"):
            # make sure we always get dict outputs
            self.vit.config.return_dict = True
            # if caller asked for attentions, enable it
            if kwargs.get("output_attentions", False):
                self.vit.config.output_attentions = True

            # HF DeiT usually wants pixel_values=
            try:
                out = self.vit(pixel_values=x, **kwargs)
            except TypeError:
                # fallback for models that still accept x
                out = self.vit(x, **kwargs)

            cls = out.last_hidden_state[:, 0]
            return self.drop(cls)

        # timm style models
        if hasattr(self.vit, "forward_features"):
            feats = self.vit.forward_features(x, **kwargs)
            if isinstance(feats, dict):
                t = feats.get("x", None)
                if t is None:
                    # pick any 2D tensor
                    for v in feats.values():
                        if isinstance(v, torch.Tensor) and v.ndim == 2:
                            t = v
                            break
                return self.drop(t)
            if feats.ndim == 3:
                return self.drop(feats[:, 0])
            return self.drop(feats)

        raise ValueError("Unsupported ViT model for feature extraction")




# -------------------------------------------------
# Attention fusion blocks used inside hybrids
# -------------------------------------------------

class SqueezeExcitation(nn.Module):
    """
    Lightweight channel attention used to reweight fused CNN/ViT features.
    """
    def __init__(self, channels: int, ratio: int = 8):
        super().__init__()
        hidden = max(1, channels // ratio)
        self.avg = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Linear(channels, hidden, bias=False)
        self.fc2 = nn.Linear(hidden, channels, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: [B, C, H, W]
        b, c, _, _ = x.shape
        y = self.avg(x).view(b, c)               # [B, C]
        y = F.relu(self.fc1(y), inplace=True)
        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)
        return x * y


# -------------------------------------------------
# Parallel fusion hybrid
# -------------------------------------------------

class ParallelHybridClassifier(nn.Module):
    """
    Parallel fusion hybrid:
    - Extract CNN features and ViT features independently
    - Concatenate
    - Apply SE attention and MLP head
    """
    def __init__(
        self,
        cnn_extractor: nn.Module,
        vit_extractor: nn.Module,
        num_labels: int = 1,
        se_ratio: int = 8,
        dropout1: float = 0.4,
        dropout2: float = 0.15,
        loss_fn: nn.Module = None,
        use_logits: bool = True,
        feat_dim: int = None,
    ):
        super().__init__()
        self.cnn_feat = cnn_extractor
        self.vit_feat = vit_extractor
        self.use_logits = use_logits
        self.loss_fn = loss_fn or nn.BCEWithLogitsLoss()

        # store head config for lazy construction
        self._head_cfg = dict(
            num_labels=num_labels,
            se_ratio=se_ratio,
            d1=dropout1,
            d2=dropout2,
        )
        self._head_built = feat_dim is not None
        if self._head_built:
            self._build_head(feat_dim, **self._head_cfg)

    def _build_head(self, feat_dim: int, num_labels: int, se_ratio: int, d1: float, d2: float):
        self.se = SqueezeExcitation(feat_dim, ratio=se_ratio)
        self.head = nn.Sequential(
            nn.Linear(feat_dim, 512),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(512),
            nn.Dropout(d1),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(256),
            nn.Dropout(d2),
            nn.Linear(256, num_labels),
        )
        self._head_built = True

    def forward(self, pixel_values: torch.Tensor, labels: torch.Tensor = None):
        f_cnn = self.cnn_feat(pixel_values)      # [B, Cc]
        f_vit = self.vit_feat(pixel_values)      # [B, Cv]
        fused = torch.cat([f_cnn, f_vit], dim=1) # [B, Cc+Cv]

        if not self._head_built:
            self._build_head(fused.size(1), **self._head_cfg)
            # move dynamically created layers to correct device
            dev = fused.device
            self.se = self.se.to(dev)
            self.head = self.head.to(dev)

        x = fused.view(fused.size(0), fused.size(1), 1, 1)  # [B, C, 1, 1]
        x = self.se(x).view(fused.size(0), -1)              # [B, C]
        logits = self.head(x)                               # [B, 1] or [B, num_labels]

        out = {"logits": logits}
        if labels is not None:
            y = labels.float().view(-1, 1) if logits.size(1) == 1 else labels.float()
            out["loss"] = self.loss_fn(logits, y)
        if not self.use_logits:
            out["probs"] = torch.sigmoid(logits)
        return out


# -------------------------------------------------
# Sequential fusion hybrid
# -------------------------------------------------

class SequentialHybridClassifier(nn.Module):
    """
    Sequential fusion hybrid:
    - CNN backbone produces a high-dim vector
    - Project that vector to a 32x32 spatial map
    - Upsample to a pseudo image and feed it into the ViT
    - Then apply SE + head

    This simulates "CNN then ViT" information flow.
    """
    def __init__(
        self,
        cnn_extractor: nn.Module,
        vit_extractor: nn.Module,
        num_labels: int = 1,
        se_ratio: int = 8,
        dropout1: float = 0.4,
        dropout2: float = 0.15,
        loss_fn: nn.Module = None,
        use_logits: bool = True,
        feat_dim: int = None,
    ):
        super().__init__()
        self.cnn_feat = cnn_extractor
        self.vit_feat = vit_extractor
        self.channel_reduction = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=1)

        # Projection layers for different CNN feature dimensions (DenseNet201 ~1920, Xception ~2048)
        self.proj2048 = nn.Linear(2048, 1024)
        self.proj1920 = nn.Linear(1920, 1024)

        self.use_logits = use_logits
        self.loss_fn = loss_fn or nn.BCEWithLogitsLoss()

        self._head_cfg = dict(
            num_labels=num_labels,
            se_ratio=se_ratio,
            d1=dropout1,
            d2=dropout2,
        )
        self._head_built = feat_dim is not None
        if self._head_built:
            self._build_head(feat_dim, **self._head_cfg)

    def _build_head(self, feat_dim: int, num_labels: int, se_ratio: int, d1: float, d2: float):
        self.se = SqueezeExcitation(feat_dim, ratio=se_ratio)
        self.head = nn.Sequential(
            nn.Linear(feat_dim, 512),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(512),
            nn.Dropout(d1),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(256),
            nn.Dropout(d2),
            nn.Linear(256, num_labels),
        )
        self._head_built = True

    def forward(self, pixel_values: torch.Tensor, labels: torch.Tensor = None):
        # Step 1. CNN feature vector, shape [B, C]
        cnn_vec = self.cnn_feat(pixel_values)
        C = cnn_vec.shape[1]

        # Step 2. Project to fixed width 1024
        if C == 1920:
            x = self.proj1920(cnn_vec)
        elif C == 2048:
            x = self.proj2048(cnn_vec)
        else:
            # fallback: build a generic projection on the fly
            if not hasattr(self, "proj_generic") or self.proj_generic.in_features != C:
                self.proj_generic = nn.Linear(C, 1024).to(cnn_vec.device)
            x = self.proj_generic(cnn_vec)

        # Step 3. Reshape [B, 1024] -> [B, 1, 32, 32]
        x = x.reshape(-1, 32, 32, 1).permute(0, 3, 1, 2).contiguous()

        # Step 4. Upsample to ViT resolution [B, 1, 224, 224] then 1x1 conv to 3 channels
        fmap = F.interpolate(x, size=(224, 224), mode="bilinear", align_corners=False)
        fmap_rgb = self.channel_reduction(fmap)  # [B, 3, 224, 224]

        # Step 5. Pass through ViT feature extractor to get [B, D]
        vit_feats = self.vit_feat(fmap_rgb)

        if not self._head_built:
            self._build_head(vit_feats.size(1), **self._head_cfg)
            dev = vit_feats.device
            self.se = self.se.to(dev)
            self.head = self.head.to(dev)

        # Step 6. Channel attention + head
        v2 = vit_feats.view(vit_feats.size(0), vit_feats.size(1), 1, 1)
        v2 = self.se(v2).view(vit_feats.size(0), -1)
        logits = self.head(v2)

        out = {"logits": logits}
        if labels is not None:
            y = labels.float().view(-1, 1) if logits.size(1) == 1 else labels.float()
            out["loss"] = self.loss_fn(logits, y)
        if not self.use_logits:
            out["probs"] = torch.sigmoid(logits)
        return out
