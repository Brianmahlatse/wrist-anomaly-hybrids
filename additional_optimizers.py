# -*- coding: utf-8 -*-
"""additional_optimizers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nmoSn387MuK1Q2uN3fVjB8VkIlSql9he
"""

"""

AdaBoB optimizer implementation.

AdaBoB is a dynamically bounded adaptive gradient method that
uses belief in the observed gradients and progressively
constrains the effective step size over training steps. It
aims to stabilize convergence while retaining fast early
adaptation.

Reference:
Q. Xiang, X. Wang, Y. Song, L. Lei.
"Dynamic Bound Adaptive Gradient Methods with Belief in Observed Gradients."
Pattern Recognition, vol. 168, 2025.
doi:10.1016/j.patcog.2025.111819

This implementation is included solely to support experimental
reproducibility of the results reported in this work. We do not
claim authorship of the AdaBoB algorithm.
"""

import math
import torch
from torch.optim.optimizer import Optimizer


class AdaBoB(Optimizer):
    """
    AdaBoB optimizer.

    Arguments:
        params: iterable of parameters to optimize
        lr (float): initial learning rate
        betas (tuple[float, float]): decay rates for first and second moments
        final_lr (float): target effective learning rate the method
                          approaches over time
        gamma (float): rate at which the bounds tighten toward final_lr
        eps (float): numerical stability term
        weight_decay (float): L2 weight decay
        amsbound (bool): if True, cap the second moment using the
                         running maximum (AMSBound-style)

    Behavior (high level):
    - Tracks an exponential moving average of gradients (exp_avg)
      and of their believed "residual"/stabilized magnitude
      (exp_avg_sq).
    - Computes an Adam-like update direction.
    - Constructs a per-parameter step size tensor.
    - Clamps that step size between dynamic lower and upper bounds
      that shrink toward `final_lr` as `step` grows.
    """

    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas=(0.9, 0.999),
        final_lr: float = 0.1,
        gamma: float = 1e-3,
        eps: float = 1e-8,
        weight_decay: float = 0.0,
        amsbound: bool = False,
    ):
        # Basic argument validation for safety
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Invalid beta[0]: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Invalid beta[1]: {betas[1]}")
        if not 0.0 <= final_lr:
            raise ValueError(f"Invalid final learning rate: {final_lr}")
        if not 0.0 <= gamma < 1.0:
            raise ValueError(f"Invalid gamma: {gamma}")

        defaults = dict(
            lr=lr,
            betas=betas,
            final_lr=final_lr,
            gamma=gamma,
            eps=eps,
            weight_decay=weight_decay,
            amsbound=amsbound,
        )
        super().__init__(params, defaults)

        # Store initial lr for each param group so we can compute
        # the dynamic bounds relative to that base
        self.base_lrs = [group["lr"] for group in self.param_groups]

    def __setstate__(self, state):
        super().__setstate__(state)
        # make sure 'amsbound' exists even if loading older checkpoints
        for group in self.param_groups:
            group.setdefault("amsbound", False)

    @torch.no_grad()
    def step(self, closure=None):
        """
        Performs a single optimization step.
        """
        loss = None
        if closure is not None:
            loss = closure()

        # Loop over parameter groups and their base learning rates
        for group, base_lr in zip(self.param_groups, self.base_lrs):
            beta1, beta2 = group["betas"]
            amsbound = group["amsbound"]

            for p in group["params"]:
                if p.grad is None:
                    continue

                grad = p.grad.data
                if grad.is_sparse:
                    # sparse gradients are not supported
                    raise RuntimeError(
                        "AdaBoB does not support sparse gradients."
                    )

                state = self.state[p]

                # State initialization
                if len(state) == 0:
                    state["step"] = 0
                    # First moment (EMA of gradient)
                    state["exp_avg"] = torch.zeros_like(p.data)
                    # Second moment / "belief" tracking
                    state["exp_avg_sq"] = torch.zeros_like(p.data)
                    if amsbound:
                        # Track max of second moment if AMSBound-style cap is active
                        state["max_exp_avg_sq"] = torch.zeros_like(p.data)

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]

                # Increment time step
                state["step"] += 1
                t = state["step"]

                # Apply weight decay on the gradient if requested
                if group["weight_decay"] != 0:
                    grad = grad.add(p.data, alpha=group["weight_decay"])

                # Update first moment (like Adam)
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)

                # Belief-style residual between raw grad and smoothed grad
                grad_residual = grad - exp_avg

                # Update second moment using residual
                exp_avg_sq.mul_(beta2).addcmul_(
                    grad_residual,
                    grad_residual,
                    value=1 - beta2,
                )

                # Normalization denominator
                if amsbound:
                    max_exp_avg_sq = state["max_exp_avg_sq"]
                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)
                    denom = max_exp_avg_sq.sqrt().add_(group["eps"])
                else:
                    denom = exp_avg_sq.sqrt().add_(group["eps"])

                # Bias corrections (Adam-style)
                bias_correction1 = 1 - beta1 ** t
                bias_correction2 = 1 - beta2 ** t

                # Base step size before bounding
                # Note: AdaBoB scales lr by 1/sqrt(t) for additional stabilisation
                step_size = (
                    (group["lr"] / math.sqrt(t))
                    * (math.sqrt(bias_correction2) / bias_correction1)
                )

                # Compute dynamic lower/upper bounds for this step
                # The bounds tighten toward final_lr over time
                final_lr = group["final_lr"] * group["lr"] / base_lr
                lower_bound = final_lr * (1 - 1 / (group["gamma"] * t + 1))
                upper_bound = final_lr * (1 + 1 / (group["gamma"] * t))

                # Build a step tensor aligned with denom shape,
                # then clamp to [lower_bound, upper_bound]
                step_tensor = torch.full_like(denom, step_size)
                step_tensor.div_(denom)
                step_tensor.clamp_(lower_bound, upper_bound)

                # Parameter update in the direction of exp_avg
                p.data.add_(exp_avg, alpha=-step_tensor)

        return loss