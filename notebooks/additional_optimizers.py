{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3822b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "AdaBoB optimizer implementation.\n",
    "\n",
    "AdaBoB is a dynamically bounded adaptive gradient method that\n",
    "uses belief in the observed gradients and progressively\n",
    "constrains the effective step size over training steps. It\n",
    "aims to stabilize convergence while retaining fast early\n",
    "adaptation.\n",
    "\n",
    "Reference:\n",
    "Q. Xiang, X. Wang, Y. Song, L. Lei.\n",
    "\"Dynamic Bound Adaptive Gradient Methods with Belief in Observed Gradients.\"\n",
    "Pattern Recognition, vol. 168, 2025.\n",
    "doi:10.1016/j.patcog.2025.111819\n",
    "\n",
    "This implementation is included solely to support experimental\n",
    "reproducibility of the results reported in this work. We do not\n",
    "claim authorship of the AdaBoB algorithm.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class AdaBoB(Optimizer):\n",
    "    \"\"\"\n",
    "    AdaBoB optimizer.\n",
    "\n",
    "    Arguments:\n",
    "        params: iterable of parameters to optimize\n",
    "        lr (float): initial learning rate\n",
    "        betas (tuple[float, float]): decay rates for first and second moments\n",
    "        final_lr (float): target effective learning rate the method\n",
    "                          approaches over time\n",
    "        gamma (float): rate at which the bounds tighten toward final_lr\n",
    "        eps (float): numerical stability term\n",
    "        weight_decay (float): L2 weight decay\n",
    "        amsbound (bool): if True, cap the second moment using the\n",
    "                         running maximum (AMSBound-style)\n",
    "\n",
    "    Behavior (high level):\n",
    "    - Tracks an exponential moving average of gradients (exp_avg)\n",
    "      and of their believed \"residual\"/stabilized magnitude\n",
    "      (exp_avg_sq).\n",
    "    - Computes an Adam-like update direction.\n",
    "    - Constructs a per-parameter step size tensor.\n",
    "    - Clamps that step size between dynamic lower and upper bounds\n",
    "      that shrink toward `final_lr` as `step` grows.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        betas=(0.9, 0.999),\n",
    "        final_lr: float = 0.1,\n",
    "        gamma: float = 1e-3,\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: float = 0.0,\n",
    "        amsbound: bool = False,\n",
    "    ):\n",
    "        # Basic argument validation for safety\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta[0]: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta[1]: {betas[1]}\")\n",
    "        if not 0.0 <= final_lr:\n",
    "            raise ValueError(f\"Invalid final learning rate: {final_lr}\")\n",
    "        if not 0.0 <= gamma < 1.0:\n",
    "            raise ValueError(f\"Invalid gamma: {gamma}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            final_lr=final_lr,\n",
    "            gamma=gamma,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            amsbound=amsbound,\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        # Store initial lr for each param group so we can compute\n",
    "        # the dynamic bounds relative to that base\n",
    "        self.base_lrs = [group[\"lr\"] for group in self.param_groups]\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        # make sure 'amsbound' exists even if loading older checkpoints\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"amsbound\", False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        # Loop over parameter groups and their base learning rates\n",
    "        for group, base_lr in zip(self.param_groups, self.base_lrs):\n",
    "            beta1, beta2 = group[\"betas\"]\n",
    "            amsbound = group[\"amsbound\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    # sparse gradients are not supported\n",
    "                    raise RuntimeError(\n",
    "                        \"AdaBoB does not support sparse gradients.\"\n",
    "                    )\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # First moment (EMA of gradient)\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Second moment / \"belief\" tracking\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "                    if amsbound:\n",
    "                        # Track max of second moment if AMSBound-style cap is active\n",
    "                        state[\"max_exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg = state[\"exp_avg\"]\n",
    "                exp_avg_sq = state[\"exp_avg_sq\"]\n",
    "\n",
    "                # Increment time step\n",
    "                state[\"step\"] += 1\n",
    "                t = state[\"step\"]\n",
    "\n",
    "                # Apply weight decay on the gradient if requested\n",
    "                if group[\"weight_decay\"] != 0:\n",
    "                    grad = grad.add(p.data, alpha=group[\"weight_decay\"])\n",
    "\n",
    "                # Update first moment (like Adam)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                # Belief-style residual between raw grad and smoothed grad\n",
    "                grad_residual = grad - exp_avg\n",
    "\n",
    "                # Update second moment using residual\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(\n",
    "                    grad_residual,\n",
    "                    grad_residual,\n",
    "                    value=1 - beta2,\n",
    "                )\n",
    "\n",
    "                # Normalization denominator\n",
    "                if amsbound:\n",
    "                    max_exp_avg_sq = state[\"max_exp_avg_sq\"]\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                # Bias corrections (Adam-style)\n",
    "                bias_correction1 = 1 - beta1 ** t\n",
    "                bias_correction2 = 1 - beta2 ** t\n",
    "\n",
    "                # Base step size before bounding\n",
    "                # Note: AdaBoB scales lr by 1/sqrt(t) for additional stabilisation\n",
    "                step_size = (\n",
    "                    (group[\"lr\"] / math.sqrt(t))\n",
    "                    * (math.sqrt(bias_correction2) / bias_correction1)\n",
    "                )\n",
    "\n",
    "                # Compute dynamic lower/upper bounds for this step\n",
    "                # The bounds tighten toward final_lr over time\n",
    "                final_lr = group[\"final_lr\"] * group[\"lr\"] / base_lr\n",
    "                lower_bound = final_lr * (1 - 1 / (group[\"gamma\"] * t + 1))\n",
    "                upper_bound = final_lr * (1 + 1 / (group[\"gamma\"] * t))\n",
    "\n",
    "                # Build a step tensor aligned with denom shape,\n",
    "                # then clamp to [lower_bound, upper_bound]\n",
    "                step_tensor = torch.full_like(denom, step_size)\n",
    "                step_tensor.div_(denom)\n",
    "                step_tensor.clamp_(lower_bound, upper_bound)\n",
    "\n",
    "                # Parameter update in the direction of exp_avg\n",
    "                p.data.add_(exp_avg, alpha=-step_tensor)\n",
    "\n",
    "        return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
